---
title: "Untitled"
author: "Equipo 2, Lote 7 (Europa)"
date: "2025-01-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = FALSE)
```

# 0.WEnv
El primer paso siempre resetear el entorno de trabajo, cargar el fichero con el que se va a trabajar (en este caso "Dataset expresión genes.csv") y cargar todas las librerías que van a ser necesarias para el análisis. 

```{r}
rm(list=ls()) #resetear WEnv
thisfile_path <- file.choose() #elegir
wd_path <- sub("/[^/]+$", "", thisfile_path) #ruta de este archivo hasta el último "/" (excluyendo nombre del archivo))
setwd(wd_path) #setear wd_path como WD

set.seed(1999) #random seed
library(tidyverse) #ggplot2, dplyr, tidyr, ggpubr, readr...
library(stats) #ops. básicas de estadística
library(factoextra) #
library(pheatmap) #heatmaps
library(gtsummary) #tabla est.descriptiva
library(MASS) 
library(glmnet)
library(ggplot2)
library(gridExtra)
library(jtools)
library(car)
```


# 1.Dataset
```{r}
df <- read.csv("1_data/Dataset expresión genes.csv") # dataframe con todas las variables
               #na.strings = este arg. dice si hay alguna cadena de texto q queramos importar como NA
df_genes <- df %>% dplyr::select(starts_with("AQ_")) # df solo de los genes
df_nogenes <- df %>% dplyr::select(-starts_with("AQ_")) # df solo de los genes
```


# 2.*PCA*

##**Introducción y metodología**

El Análisis de Componentes Principales (PCA) es un método de reducción de la dimensionalidad muy utilizado, sobretodo para conjuntos de datos con relaciones lineales. El objetivo de este modelo es reducir la complejidad de un conjunto de variables observadas, identificando una serie de componentes principales que explican la mayor parte de la variabilidad en los datos originales. En este modelo se asume que cada componente se forma por la combinación lineal de todas las variables del modelo y cada una tiene una determinada carga que indica si contribuye más o menos al componente y su dirección de contribución.

En este ejemplo, el objetivo era aplicar un PCA sobre las variables de expresión génica del dataset, para luego determinar el número de componentes óptimo para explicar la mayor parte de los datos (un 70% de variaza) y darle un sentido biológico a esos componentes principales. 

Para aplicar el PCA en R, se puede usar la función prcomp() de la librería stats:

```{r}
pca.result <- prcomp(df_genes, center = TRUE, scale = TRUE )
```

##**Análisis y gráficos**
**A-Elección del número de componentes principales**

En el PCA se generan muchos componentes principales y es fundamental elegir el número óptimo para el análisis. Existen diferentes criterios para elegir este número de componentes principales. Uno de ellos sería analizar la varianza explicada por cada componente principal y buscar el número de componentes donde la varianza acumulada sea adecuada (en este caso hemos elegido un 70%):

```{r}
eigenvalues <- get_eigenvalue(pca.result)
eigenvalues 
```

Con este método, habría que coger al menos 5 componentes principales. Otra opción es generar un scree plot y buscar el punto de allanamiento de la gráfica:


```{r}
fviz_eig(pca.result, addlabels = TRUE) 
```

En el gráfico, se observa que  el allanamiento de la gráfica ocurre en el segundo componente, ya que el componente principal explica gran parte de la varianza (52,5%) y el resto de componentes aportan mucha menos varinza. Sin embargo, como consideramos importante llegar a un 70% de varianza, decidimos trabajar con los cinco primeros componentes principales. 

**B-Análisis de las variables y los componentes**

Podemos representar, en dos dimensiones, las variables del dataset, en forma de flechas, cuya longitud indica la fuerza con la que las variables constribuyen a cada dimensión y cuya dirección indica si la asociación es positiva o negativa. 
Además, podemos establecer un código de colores, de modo que se represente simultáneamente el parámetro cos2. Este parámetro indica qué tan bien están representadas las variables por los componentes principales, en otras palabras, establece cómo de bien "caben" las variables en el espacio de menor dimensión. 

```{r}
fviz_pca_var(pca.result, col.var = "cos2", gradient.cols = c("blue","yellow", "red"), repel = TRUE, axes = c(1,2),
             title = "Cos2 de variables en PC1 y PC2")
```

En el gráfico, se puede apreciar como  casi todas las variables se ajustan bien al epacio de menor dimension,menos algunos genes como ADIPOQ o NOX5.
Además, en ese gráfico también se observa la tendencia que tienen muchas variables a asociarse negativamente y con bastante fuerza a la dimensión 2. 

Este código de colores se puede establecer también según otros criterios. Por ejemplo, se puede aplicar un algoritmo de clusterización, para intentar agrupar las variables:

```{r}
kmeans <- kmeans(t(df_genes), centers = 2)

clusters <- list()

for (i in 1:4) {
  
  cluster.plot <- fviz_pca_var(pca.result, col.var = kmeans$cluster, gradient.cols = c("blue","green", "red"), axes = c(i,i+1), 
                               legend.title ="Clusterizacion", repel = TRUE) 
  clusters[[i]] <- cluster.plot
}

todos.clusters <- grid.arrange(grobs = clusters, nrow = 3)
todos.clusters
```

En estas gráficas, se ha representado la fuerza y la dirección de las variables con distintas combinaciones de pares de dimensiones. Para la dimensión 2 se observa la tendencia que tienen las variables de asociarse de forma negativa e intensa, unna tendencia que no se aprecia para el resto de dimensiones, donde hay mucha heterogeneidad. Por otro lado, la clusterización no ha sido eficiente, ya que prácticamente todas las variales se agrupan en el mismo cluster, mientras que queda una pareja de variables en otro cluster, que coincide con las variables con un menor cos2, es decir, este grupo consiste en las variables que peor se ajustan al modelo. Sería interesante probar otros algoritmos de clusterización (Asignatura de Algoritmos, tema 4) para analizar si alguno de ellos permite una agrupación más óptima de las variables. No lo hemos estudiado porque hemos considerado que no era el objetivo de la actividad. 

Para dar un sentido y un nombre a los 5 componentes principales elegidos, hemos analizado las variables que más contribuyen a cada uno de estos componentes:

```{r}
graficos <- list()
for (i in 1:5) {
  grafico <- fviz_contrib(pca.result, choice ="var", axes = i, top = 5) 
  graficos[[i]] <- grafico
}

graficos_varianza <- grid.arrange(grobs = graficos, nrow = 3)
```
Tras una búsqueda de estos genes en bases de datos como GenBank, se ha determinado que:
1) El componente 1 está relacionado con alteraciones en procesos inflamatorios, de respuesta inmune y estrés celular. 

2) El componente 2 está relacionado con alteraciones en procesos de inmunorregulación. 

3) El componente 3 está relacionado con alteraciones en el metabolismo energético y de señalización de estrés celular. 

4) El componente 4 está relacionado con alteraciones en la inmunidad innata y la homeostasis. 

5) El componente 5 está relacionando con alteraciones en la inflamación y la regeneración celular. 


**C-Análisis de los individuos y los componentes**

Por un lado, puesto que para las variables aplicamos una clusterización k-means, también aplicamos esta clusterización para los pacientes:

```{r}
kmeans2 <- kmeans(df_genes, centers = 3)
fviz_cluster(kmeans2, df_genes) 
```

En el gráfico se pueden observar muchos solapamientos entre los clusters. Esto se puede deber a que, aunque los pacientes tengan fenotipos diversos, como los genes pueden estar implicados en diferentes rutas metabolicas y procesos tumorales, los pacientes pueden tener diferentes enfermedades o tipos de cáncer, pero un patrón génico similar o coincidente en  algunos genes. 

También analizamos la distribución de los pacientes en las dos primeras dimensiones, analizando el parámetro cos2:

```{r}
fviz_pca_ind(pca.result, col.ind = "cos2", gradient.cols = c("blue","yellow", "red"), repel = TRUE)

```

En el gráfico observamos que muchos pacientes se ajustan bien  al nuevo espacio reducido, con algunas excepciones, como los pacientes 10,27, 14 ó 50. 

También clasificamos a los pacientes según si tenían metástasis o no, codificando para ello en el dataset una nueva variable a partir de la variable original "extension". Una vez codificada esta variable, analizamos en un gráfico de puntos ,si tomando diferentes parejas de dimensiones, se podían separar estos dos grupos de pacientes:

```{r}
df$metastasisnosi <- as.factor(ifelse( df$extension == "metastasico", "metastasis", "no_metastasis"))

df_pca <- as.data.frame(pca.result$x)

df_pca <- df_pca[1:5] 

componentes <- c("PC1", "PC2", "PC3", "PC4", "PC5")
componentes.plots <- list()

for (i in 1:(length(componentes) - 1))  {
  grafico <- ggplot(df_pca, aes_string(x =componentes[i], y = componentes[i+1], color = df$metastasisnosi))+geom_point(size = 3)
  componentes.plots[[i]] <- grafico
}

grid.arrange(grobs = componentes.plots, nrow = 3)

```

No parece que ninguna gráfica, es decir, que ninguna de las parejas de dimensiones estudiadas, permitan separar a los pacientes según si tienen metástasis o no. Esto podría deberse a que el PCA no es el método de reducción de dimensionalida más adecuado para la separación de los pacientes según si tienen metástasis o no. 


Mas cosas de la clusterizacion 
### Resultado del clustering
Tenemos 3 clusters: 2 con 2 variables y otro con 42. Tras buscar 2 genes de cada Cluster en GenBank, nombramos los clusters de la siguiente forma:

- Cluster ADIPOQ y NOX5: **relacionado con el metabolismo celular** (NOX5 está relacionado con el transporte de protones transmembrana) y hormonal (ADIPOQ vinculando a este proceso en el tejido adiposo)
- Cluster CCL5 y TGFB1: **relacionado con el sistema inmune**, ya que CCL5 es una quimiocina y TGFB1 está íntimamente relacionado con sistemas como los interferones.
- Cluster general: contiene genes muy variados y con muchas funciones, como ARG1 (que codifica la Arginasa, encargada de catabolizar el aminoácido Arginina),  LOX5 (que codifica la Lipooxigenasa 5) o MAPK1 (encargada de transducir señales de membrana al núcleo para la transcripción de genes de respuesta a estas). **Por eso lo hemos llamado cluster general**.


# 2.*Tabla descriptiva*

```{r}

```


# *3.Modelo predictivo Reg.Logística*

El objetivo de este análisis es evaluar cómo influyen una serie de variables independientes sobre una variable dependiente. En este caso la variable dependiente es "metastasisino", una variable categórica con dos niveles, que determina si los pacientes tienen metástasis o no. Por otro lado, las variables independientes son los cinco componentes principales analizados en el PCA, junto con el resto de variables del dataset original (sin contar con los datos de expresión génica, puesto que ya hemos reducido la dimensionalidad y los tenemos representados con los componentes principales).

Tenemos demasiadas variables independientes por lo que, antes de aplicar la regresión logística, decidimos usar un modelo de regularización, para reducir el número de variables. Al principio, intentamos usar LASSO,  un modelo de regularización que aplica una penalización sobre las variables y permite seleccionar solo aquellas variables con un coeficiente que tras la penalización tienen un valor mayor a 0, que son las variables más importantes a la hora de hacer el análisis. Sin embargo, esto nos salía mal ****indicar bien las razones

Al finar decidimos usar ElasticNet *****explicar bien todo

Antes de poder aplicarr el modelo de regularización, generamos un dataframe con todas las variables (salvo las de expresión génica, que hemos proyectado en un espacio de menor dimensión en forma de componentes) y las convertimos a valores numéricos, ya que es el tipo de dato que necesita las funciones glmnet() y cv.glmnet()
```{r}
# las variables factor están en tipo character, tengo que convertirlas a numérico pq glmnet no admite tipo factor
# casteo a numérico variables char
df_nogenes <- df %>% dplyr::select(-starts_with("AQ_"), -X, -id, -extension) # df de vars q no son genes

df_char <- df_nogenes[ ,sapply(df_nogenes, is.character)]# guardamos las vars factor en un vector lógico
df_char$antiemesis <- as.character(ifelse(df$corticoides=="antiemesis", "1", "0"))
df_char$corticoides <- as.character(ifelse(df$corticoides=="si", "1", "0"))

df_nochar <- df_nogenes[, sapply(df_nogenes, is.numeric)]
df_nochar <- as.data.frame(scale(df_nochar, center=TRUE, scale=TRUE))
# convierto variables tipo char a factor y luego a numeros para codificarlas de manera que glmnet las entienda
for (i in colnames(df_char)) {
  df_char[[i]] <-as.numeric(as.factor(df_char[[i]]))
}
# y concateno df_char, df_nochar y df5 pcs en df_lasso
df_lasso <- cbind(df_char, df_nochar)

# ahora tb dejo bien formateada la variable respuesta
df$metastasis = as.factor(as.numeric(ifelse(df$extension == "metastasico", 1, 0)))

# defino variables para df_lasso
cols = c(colnames(df_lasso))
x <- df_lasso[, cols]
y <- df$metastasis

# convertir a matriz para incluir interacciones entre todas las variables
#formula <- as.formula(paste("y ~", paste(names(x), collapse = " * "))) # Crear una matriz de diseño con interacciones entre todas las variables
```


#### ridge
```{r}
# construyo un grid para lambda (para el lambda optimo??)
grid <- 10^seq(1.5, -1.5, length=100)
ridge <- glmnet(x, y, alpha=0, lambda=grid, family="binomial")
dim(coef(ridge))
print(coef(ridge))

plot(ridge, label=TRUE) # lambda plot
plot(ridge, xvar="lambda", label=TRUE) #log lambda plot
```

estos plots nos dan una idea gráfica del valor óptimo de lambda para ridge
```{r}
x_matrix <- as.matrix(x)
y_matrix <- as.matrix(y)
# hago un m
#modelo de cross validation
ridge_cv <- cv.glmnet(x_matrix, y_matrix, alpha=0, lambda=grid, family="binomial")
lambda_min <- ridge_cv$lambda.min # este es el lambda que menor error nos da para el modelo, me lo guardo en un objeto
# y lo uso para generar modelo de ridge
ridge_cv <- glmnet(x_matrix, y_matrix, alpha=0, lambda = lambda_min, family="binomial")
coef(ridge_cv)
```

Todas las variables tienen valores muy bajos... Creo que sería ideal eliminar algunas

#### lasso
```{r}
grid <- 10^seq(-4,1, lenght=100)
lasso <- glmnet(x, y, alpha=1, lambda=grid, family="binomial")
dim(coef(lasso))
print(coef(lasso))

plot(lasso, label=TRUE) # lambda plot
plot(lasso, xvar="lambda", label=TRUE) #log lambda plot
```

estos plots nos dan una idea gráfica del valor óptimo de lambda para ridge
```{r}
# hago un modelo de cross validation
grid <- 10^seq(-3,0, lenght=100)
lasso_cv <- cv.glmnet(x_matrix, y_matrix, alpha=1, lambda=grid, family="binomial")
plot(lasso_cv)
```

El lambda min me dejará solo una variable (intercepto), pero podemos usar otra regla: en vez de el lambda mínimo, usamos el lambda "within one standard error from the minimum" --> lambda que minimiza el CV error + 1 SE (si no, el valor de lambda mínimo se carga todas las variables)

```{r}
lambda_min <- lasso_cv$lambda.min
lambda_1se <- lasso_cv$lambda.1se # este es el lambda que menor error nos da para el modelo, me lo guardo en un objeto
# y lo uso para generar modelo de ridge
lasso_cv <- glmnet(x, y, alpha=1, lambda = lambda_1se, family="binomial")
coef(lasso_cv)
```

#### elastic net

```{r}
seq_alpha <- seq(0,1,by=0.01)
grid <- 10^seq(3, -3, length=100) # lo hago de + a - pq el lambda óptimo  es negativo (ver plots iniciales de lasso)

best_alpha <- NULL
best_lambda <- NULL
min_error <- Inf

# encontrar el mejor alpha
for (alpha in seq_alpha) {
  enet_cv <- cv.glmnet(x_matrix, y_matrix, alpha=alpha, lambda=grid, family="binomial")
  # Obtener valor de lambda óptimo elegido automáticamente
  lambda_min<-enet_cv$lambda.min
  #Obtener el error de validación cruzada mínimo
  cv_error <- min(enet_cv$cvm)
  #Actualizar el mejor alpha y lambda si se encuentra un error de cv menor
  if (cv_error < min_error) {
    min_error <- cv_error
    best_alpha <- alpha
    best_lambda <- lambda_min
  }}

# Imprimir el mejor alpha y lambda encontrados tras el bucle
cat("Mejor alpha:", best_alpha, "\n")
cat("Mejor lambda:", best_lambda, "\n")
```


```{r ejecutar elasticnet}
grid <- 10^seq(0, -2, length=100)
# hago un modelo de cross validation
enet_cv <- cv.glmnet(x_matrix, y_matrix, alpha=0.67, lambda=grid, family="binomial")
plot(enet_cv)
lambda_1se <- enet_cv$lambda.1se
lambda_1se
```

```{r}
enet_cv <- glmnet(x_matrix, y_matrix, alpha=0.36, lambda=10^-1, family="binomial")
coefs_enet <- as.data.frame(as.matrix(coef(enet_cv))) %>% filter(s0!=0)
coefs_enet <- coefs_enet %>% slice_tail(n = 18) # quitamos el intercept
reg_vars <- rownames(coefs_enet)
reg_vars <- reg_vars[reg_vars != "antiemesis"]
```

Variables de regularización están en un vector `reg_vars`.

Una vez seleccionadas las variables del dataset original para el modelo de regresión logística, añadimos también los terciles de los cinco primeros componentes principales, en forma de variables categóricas que indian a qué tercil de cada componente pertenece cada paciente. Para ello, primero se calcularon los terciles de los componentes con la función quantile() y luego se recodificaron las variables de los componentes principales mediante la función cut()

```{r}
#Terciles 
tercilPC1 <- quantile(df_pca$PC1, probs = c(1/3,2/3,1))
tercilPC2 <- quantile(df_pca$PC2,probs = c(1/3,2/3,1))
tercilPC3 <- quantile(df_pca$PC3, probs = c(1/3,2/3,1))
tercilPC4 <- quantile(df_pca$PC4, probs = c(1/3,2/3,1))
tercilPC5 <- quantile(df_pca$PC5, probs = c(1/3,2/3,1))

df_regresion <- df%>% dplyr::select(reg_vars)

#Variable tercil > forma categorica
df_regresion$TPC1   <- as.factor(cut(df_pca$PC1, c(-Inf, tercilPC1[1], tercilPC1[2], Inf), labels = c("T1", "T2", "T3")))

df_regresion$TPC2   <- as.factor(cut(df_pca$PC2, c(-Inf, tercilPC2[1], tercilPC2[2], Inf), labels = c("T1", "T2", "T3")))

df_regresion$TPC3   <- as.factor(cut(df_pca$PC3, c(-Inf, tercilPC3[1], tercilPC3[2], Inf), labels = c("T1", "T2", "T3")))

df_regresion$TPC4   <- as.factor(cut(df_pca$PC4, c(-Inf, tercilPC4[1], tercilPC4[2], Inf), labels = c("T1", "T2", "T3")))

df_regresion$TPC5   <- as.factor(cut(df_pca$PC5, c(-Inf, tercilPC5[1], tercilPC5[2], Inf), labels = c("T1", "T2", "T3")))

#Tambien hay que añadir la variable categorica
df_regresion$metastasis <- as.factor(df$metastasis)

#El resto de variables categoricas hay que pasarlas a factor
df_regresion$sexo <- as.factor(df_regresion$sexo)
df_regresion$ETE <- as.factor(df_regresion$ETE)
df_regresion$neumopatia <- as.factor(df_regresion$neumopatia)
df_regresion$hepatopatia <- as.factor(df_regresion$hepatopatia)
df_regresion$ITU <- as.factor(df_regresion$ITU)
df_regresion$renal  <- as.factor(df_regresion$renal)
df_regresion$neuropatia <- as.factor(df_regresion$neuropatia)
df_regresion$corticoides <- as.factor(df_regresion$corticoides)
df_regresion$secrecion <- as.factor(df_regresion$secrecion)
df_regresion$dolor_garg <- as.factor(df_regresion$dolor_garg)
df_regresion$anosmia <- as.factor(df_regresion$anosmia)
df_regresion$dolor_abdo <- as.factor(df_regresion$dolor_abdo)

```

Una vez tuvimos en un dataframe todas las variables del dataset original que hemos seleccionado para la regresion logística, junto con los terciles de los cinco primeros componentes codificados en forma de factor, hicimos la regresión logística:

```{r}
reg_vars <- c(reg_vars, "TPC1", "TPC2", "TPC3","TPC4","TPC5")
formula <- as.formula(paste("metastasis ~", paste(reg_vars, collapse = "+")))

modelo_regresion <- glm(formula, data = df_regresion, family = "binomial")
```

Para poder interpretar los resultados del modelo de regresión logística, necesitamos sacar los Odds-Ratios (OR), ya que los coeficientes no son directamente interpretables. Para ello, hay que obtener la forma exponencial de dichos coeficientes. En estos OR, lo que se obtiene es el efecto, positivo o negativo, que las variables independientes tienen sobre la clase de interés de la variable dependiente (en este caso, 1, es decir, tener metástasis) con respecto a la clase de referencia (en este caso 0, es decir, no tener metástasis). También hay que tener en cuenta si este efecto es significativo o no, lo cual se puede saber a través del pvalor o analizando los intervalos de confianza. 

```{r}
exp(coef(modelo_regresion)) #OR
exp(confint(modelo_regresion))
summ(modelo_regresion) #pvalor

#Salen cosas muy raras, vamos a ver la colinealidad 
vif(modelo_regresion)

#habria que quitar aquellas variables con un VIF mayor de 5, como sexo, neumopatia, neuropatia, coticoides, secrecion, dolo_abdio, chol, hierro, igN, cpk (realmente los terciles tambien tienen colinealidad pero es raro porque se suponen que vienen de un PCA)

variables_a_eliminar <- c("sexo", "neumopatia", "neuropatia", "corticoides", "secrecion", "dolor_abdo", "chol", "hierro", "igN", "cpk")

# Eliminar las variables específicas de la fórmula
reg_vars_modificadas <- setdiff(reg_vars, variables_a_eliminar)

# Crear la nueva fórmula con las variables modificadas

formula_modificada <- as.formula(paste("metastasis ~", paste(reg_vars_modificadas, collapse = "+")))


modelo_regresion2 <- glm(formula_modificada, data = df_regresion, family = "binomial")

summ(modelo_regresion2) #ahora salen cosas mas normales
exp(confint(modelo_regresion2))

vif(modelo_regresion2) #ahora ya no hay tanto problema de colinealidad, aunque algunas variables siguen sin salir bien del todo. 

```


Ahora hay que calcular terciles de las PCs (`df_5pcs`)
```{r}
for (i in )
```


# 4.Tabla descriptiva
```{r}
for (i)
```


variables dummy
$$
baseline = T1 \rightarrow\text{para los 5 PCs} \\

metastasis = PC1T2\times\beta_{PC1T2} + PC1T3\times\beta_{PC1T3} + \\
PC2T2\times\beta_{PC1T2} + PC2T3\times\beta_{PC1T3} + \\
PC3T2\times\beta_{PC1T2} + PC3T3\times\beta_{PC1T3} + \\
PC4T2\times\beta_{PC1T2} + PC4T3\times\beta_{PC1T3} + \\
PC5T2\times\beta_{PC1T2} + PC5T3\times\beta_{PC1T3}
$$
```


```{r}





## terciles PCs
```{r}
# terciles PC1 --> factor var
df_rlog$`Terciles PC1` <- quantile()

# terciles PC2 (lo mismo que antes)
# terciles PC3 (lo mismo que antes)
# terciles PC4 (lo mismo que antes)
# terciles PC5 (lo mismo que antes)
```

