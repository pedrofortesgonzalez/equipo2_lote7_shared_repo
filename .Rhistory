resultado_prueba <- t.test(imc ~ sexo, data = df, equal = FALSE)
print(resultado_prueba)
resultado_prueba$p.value #Se rechaza la hipótesis nula.
#Si se pusiera equal = TRUE, sería la prueba t-student
########################################################################################
#### ---- CONTRASTE DE HIPÓTESIS entre la edad y pacientes con estado de salud ---- ####
########################################################################################
library(ggplot2)
#edad: variable continua
# estado de salud: variable  categórica con tres niveles:
table(df$estado_salud)
#n son pacientes independientes y mayor de 30
#Hipótesis nula y  alternativa:
# H0: bueno[edad] = malo[edad] = regular[edad] -> p≥0.05
# H1: bueno[edad] ≠/= malo[edad] ≠/= regular[edad] -> p<0.05
#Si  la distribución es normal, se aplica ANOVA, sino Kruskal-Wallis
#Se  comprueba la distribución normal con el test de shapiro-wilk:
#H0: los datos siguen una distribución normal p≥0.05
#H1: los datos no siguen una distribución normal p<0.05
shapiro.test(df$edad) #La variable tiene una distribución normal
#Como la distribución es normal, se usa el Test de ANOVA
anova <- aov(edad ~ estado_salud, data = df)
summary(anova) #No se puede  descartar la hipótesis nula
#Con esto no vemos el pvalor para cada pareja de valores.
#Esto se puede analizar gráficamente:
ggplot(data = df, aes(x = estado_salud , y = edad, fill = estado_salud)) +
geom_boxplot() +
geom_jitter(width = 0.2, alpha=0.5) +
theme_classic()
#O con el test de comparaciones múltiples
TukeyHSD(anova)
##################################################################################################
#### ---- CONTRASTE DE HIPÓTESIS entre el tipo de tratamiento y el tiempo de tratamiento ---- ####
##################################################################################################
#Son dos variables categóricas: chi cuadrado ó prueba de Fisher
#Hipótesis nula y alternativa:
# H0: A[tiempo] = B[tiempo] = C[tiempo] -> p≥0.05
# H1: A[tiempo] ≠/= B[tiempo] ≠/= C[tiempo] -> p<0.05
#Se comprueban si las probabilidades son mayores de 5%
prop.table(table(df$tratamiento, df$tiempo))*100 # sacar el % y ver si usar chi cuadrado o Fisher
#Se puede aplicar chi - cuadrado
chisq.test(df$tratamiento, df$tiempo)
#Si hubiera que aplicar Fisher, el código sería:
fisher.test(df$tratamiento, df$tiempo)
#############################################################################################################
#### ---- CONTRASTE DE HIPÓTESIS entre el país de tratamiento y el nivel de expresión de los genes ---- ####
#############################################################################################################
#Queremos ver si, para cada gen, hay una expresión diferencial dependiendo del país
#no vamos a ir de uno en uno sino iterando sobre el vector genes.
#Generamos una tabla, con la variable genes, y el p-valor, que al inicio está vacío
df_resultados <- data.frame(genes = genes, p_valor_disnormal = numeric(length(genes)),
p_valor_anova = numeric(length(genes)))
#Aplicamos el test  de shapiro-wilk a todos los genes
for  (i in 1:length(genes)) {
resultado_shapiro <- shapiro.test(df[[genes[i]]])
df_resultados$p_valor_disnormal[i] <- resultado_shapiro$p.value
}
#Como todos los genes tienen una distribución normal, aplicamos  el  test Anova
for (i in 1:length(genes)) {
resultado_anova <- aov(df[[genes[i]]] ~ pais, data = df)
df_resultados$p_valor_anova[i] <- summary(resultado_anova)[[1]]$`Pr(>F)`[1]  # Accedemos al valor p de ANOVA
}
#Test de comparaciones múltiples
for (i in 1:length(genes)) {
anova <- aov(df[[genes[i]]] ~ pais, data = df)
print(paste("Resultados de Tukey para el gen:", genes[i]))
print(TukeyHSD(anova))
}
##################################
#### ---- Tabla resumen ---- ####
##################################
library(gtsummary)
library(dplyr)
#Para  poder calcular el pvalor y visualizarlo de forma más óptima
# se pueden usar estas tablas, con el argumento add_p
tabla <- df %>%
tbl_strata(strata = tratamiento,
.tbl_fun = ~ .x %>%
tbl_summary(by = sexo,
statistic = all_continuous() ~ "{mean} ({sd})") %>%
add_p(test = list(all_continuous() ~ "t.test",
all_categorical() ~ "chisq.test"),
pvalue_fun = ~ style_pvalue(.x, digits = 3))
)
tabla <- df %>%
tbl_strata(strata=tratamiento,
.tbl_fun = ~.x %>% # es una forma de aplicar una función personalizada a las tablas que creas con el paquete, permite modificar o cambiar esa tabla después de que ha sido creada
tbl_summary(by=sexo,
statistic = all_continuous() ~ "{mean} ({sd})") %>%
add_p(test = list(all_continuous() ~ "t.test",
all_categorical() ~ "chisq.test"),
pvalue_fun = ~ style_pvalue(.x, digits = 3))
)
tabla <- df %>%
tbl_strata(
strata = tratamiento,
.tbl_fun = function(x) {
x %>%
tbl_summary(by = sexo,
statistic = all_continuous() ~ "{mean} ({sd})") %>%
add_p(test = list(all_continuous() ~ "t.test",
all_categorical() ~ "chisq.test"),
pvalue_fun = ~ style_pvalue(.x, digits = 3))
}
)
install.packages("cardx")
tabla <- df %>%
tbl_strata(
strata = tratamiento,
.tbl_fun = function(x) {
x %>%
tbl_summary(by = sexo,
statistic = all_continuous() ~ "{mean} ({sd})") %>%
add_p(test = list(all_continuous() ~ "t.test",
all_categorical() ~ "chisq.test"),
pvalue_fun = ~ style_pvalue(.x, digits = 3))
}
)
tabla <- df %>%
tbl_strata(
strata = tratamiento,
.tbl_fun = function(x) {
x %>%
tbl_summary(by = sexo,
statistic = all_continuous() ~ "{mean} ({sd})") %>%
add_p(test = list(all_continuous() ~ "t.test",
all_categorical() ~ "chisq.test"),
pvalue_fun = ~ style_pvalue(.x, digits = 3))
}
)
install.packages("MASS")
clases_predichas <- lda_pred$class #clases predichas por el modelo
library(MASS)
library(ggplot2)
#Se aplica la función ida sobre el conjunto de entrenamiento
lda_model <- lda(formula_tumor, data = df_train)
rm(list=ls())
set.seed(1995)
library(dplyr)
library(caret)
df <- read.csv("/Users/cristinalazaro/Desktop/UNIR/R/Actividad_1/actividad_1_R/Dataset expresión genes.csv")
##############################################################################
#TRATAMIENTO DE LOS DATOS: EXPRESIÓN GÉNICA SEGÚN EL TIPO DE TUMOR
##############################################################################
#Primer paso: dividir el dataset en grupo de entrenamiento y grupo de
#testing para  construir luego la matriz de confusión
#Antes de eso, del df original, nos quedamos solo con las variables genes
df_genes_tumor <- dplyr::select(df, starts_with("AQ_"), tumor)
df_genes_tumor$tumor <- as.factor(df_genes_tumor$tumor) #convertir a factor
labels_originales <- levels(df_genes_tumor$tumor) #para guardar las
#etiquetas originales
print(labels_originales)
#se ponen los dos puntos porque la librería caret también tiene la función
#select, así se indica que nos referimos a  la librería dplyr
#Usar CreateDataPartition para dividir el df en el grupo de training  y
#el grupo de testing
#Con la función se crean los índices aleatorios que pasan a ser del df de
#entrenamiento. Con p se toma el porcentaje (en tanto por uno) de ínidces
#que pasan a df train
index_train <- createDataPartition(df_genes_tumor$tumor, p = 0.8, list= FALSE)
#Ahora que ya tenemos los índices, lo pasamos a un dataframe
df_train <- df_genes_tumor[index_train, ] #solo x filas, todas las columnas
df_testing <- df_genes_tumor[-index_train, ] #todas menos x filas, todas las columnas
#Podemos ver la cantidad de pacientes que hay en cada dataframe
table(df_train$tumor)
table(df_testing$tumor)
#Escalado de datos
#Solo hay que escalar las variables numéricas.
#Con sapply, elegimos solo las variables numéricas (is.numeric)
#Las variables seleccionadas se acumulan en un nuevo dataframe (numerical_cols)
#Este es el dataframe que se escala (scaled_data)
numerical_cols <- df_train[, sapply(df_train, is.numeric)] #todas las filas
#solo las variables numéricas de df_train
scaled_data <- scale(numerical_cols) #escalado
#Ahora hay que añadir la variable categórica. Hay dos opciones,
#se puede usar mutate() de dplyr, en cuyo caso hay que pasar el resultado
#de scaled_data a dataframe primero, ya que el resultado de scale () es una
#matriz
#La otra opción es usar cbind
df_train <- cbind(scaled_data, tumor = df_train$tumor)
#Hacemos lo mismo para el df de training
numerical_cols_testing <- df_testing[ , sapply(df_testing, is.numeric)]
scaled_data_testing <- scale(numerical_cols_testing)
df_testing <- cbind(scaled_data_testing,  tumor = df_testing$tumor)
#volver a convertir a dataframe y pasar la variable categórica a factor
df_testing <- as.data.frame(df_testing)
df_train <- as.data.frame(df_train)
df_testing$tumor <- as.factor(df_testing$tumor)
df_train$tumor <- as.factor(df_train$tumor)
#Crear la fórmula sumando cada gen (esto es necesario para luego aplicar los
# métodos del discriminantes)
#Primero se seleccionan los nombres de los genes (nombres de los genes de
#de cada columna)
#La función as.formula() devuelve un objeto tipo fórmula de R, que se usa
#para análisis estadísticos
#En este caso, la fórmula establece una relación entre la variable
#dependiente (tumor) y la variable independiente (los genes)
genes <- colnames(df_train[, 1:46])
formula_tumor <- as.formula(paste("tumor ~", paste(genes, collapse = "+")))
formula_tumor
##############################################################################
#TRATAMIENTO DE LOS DATOS: EXPRESIÓN GÉNICA SEGÚN EL TIPO DE TRATAMIENTO
##############################################################################
#Volvemos a dividir el df inicial en grupo de training y grupo de testing
df_genes_tratamiento <- dplyr::select(df, starts_with("AQ_"), trat)
df_genes_tratamiento$trat <- as.factor(df_genes_tratamiento$trat)
index_train_trat <- createDataPartition(df_genes_tratamiento$trat, p = 0.8, list = FALSE)
df_train_tratamiento <- df_genes_tratamiento[index_train_trat, ]
df_test_tratamiento <- df_genes_tratamiento[-index_train_trat, ]
table(df_train_tratamiento$trat)
table(df_test_tratamiento$trat)
df_train_tratamiento_col <- df_train_tratamiento[ , sapply(df_train_tratamiento, is.numeric)]
df_train_tramaiento_scaled <- scale(df_train_tratamiento_col)
df_train_tratamiento <- cbind(df_train_tramaiento_scaled, trat = df_train_tratamiento$trat)
df_test_tratamiento_col <- df_test_tratamiento[ ,sapply(df_test_tratamiento, is.numeric)]
df_test_tratamiento_scaled <- scale(df_test_tratamiento_col)
df_test_tratamiento <- cbind(df_test_tratamiento_scaled, trat = df_test_tratamiento$trat)
genes <- colnames(df_genes_tratamiento[, 1:10])
formula_tratamiento <- as.formula(paste("trat ~ ", paste(genes, collapse = "+")))
formula_tratamiento
df_test_tratamiento <- as.data.frame(df_test_tratamiento)
df_train_tratamiento <- as.data.frame(df_train_tratamiento)
df_test_tratamiento$trat <- as.factor(df_test_tratamiento$trat)
df_train_tratamiento$trat <- as.factor(df_train_tratamiento$trat)
##############################################################################
#                             LDA (lineal)
##############################################################################
library(MASS)
library(ggplot2)
lda_model <- lda(formula_tumor, data = df_train)
lda_model$scaling
#Sobre el resultado de lda, se puede aplicar la función predict() para predecir
#la clasificación de nuevos conjuntos de datos
lda_pred <- predict(lda_model, newdata = df_testing)
lda_pred$class
#Asi tenemos las variables en modo factor, si queremos recuperar las
#clases originales:
predicciones_nombres <-  labels_originales[lda_pred$class]
print(predicciones_nombres)
print(lda_pred$class)
Como en el dataframe df_testing también tenemos las etiquetas reales,
#Como en el dataframe df_testing también tenemos las etiquetas reales,
#podemos generar una matriz de confusión
clases_predichas <- lda_pred$class #clases predichas por el modelo
clases_reales <- df_testing$tumor #clases reales de la base de datos
#Crear la matriz de confusión con confusionMatrix()
confusion_lda <- confusionMatrix(clases_predichas, clases_reales)
print(confusion_lda)
#Gráficos de clasificación
#analizar visualmente el LD1 y LD2, usando el conjunto de prueba
lda_predict_training <- predict(lda_model, newdata = df_train)
lda.data <- cbind(df_train, lda_predict_training$x)
ggplot(lda.data, aes(LD1, LD2))+
geom_point(aes(color=tumor))+theme_classic()
quartz()
ggplot(lda.data, aes(LD1, LD2))+
geom_point(aes(color=tumor))+theme_classic()
pvalues
rm(list = ls())
install.packages("xgboost")
install.packages("randomForest")
library(randomForest) #varimp
install.packages("mda")
setwd("~/Desktop/equipo2_lote7_shared_repo")
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = FALSE)
rm(list=ls()) #resetear WEnv
thisfile_path <- file.choose() #elegir
set.seed(1999) #random seed
library(tidyverse) #ggplot2, dplyr, tidyr, ggpubr, readr...
library(stats) #ops. básicas de estadística
library(factoextra) #
library(pheatmap) #heatmaps
library(gtsummary) #tabla est.descriptiva
library(MASS)
library(glmnet)
library(ggplot2)
library(gridExtra)
df <- read.csv("1_data/Dataset expresión genes.csv") # dataframe con todas las variables
#na.strings = este arg. dice si hay alguna cadena de texto q queramos importar como NA
df_genes <- df %>% dplyr::select(starts_with("AQ_")) # df solo de los genes
df_nogenes <- df %>% dplyr::select(-starts_with("AQ_")) # df solo de los genes
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = FALSE)
rm(list=ls()) #resetear WEnv
thisfile_path <- file.choose() #elegir
wd_path <- sub("/[^/]+$", "", thisfile_path) #ruta de este archivo hasta el último "/" (excluyendo nombre del archivo))
setwd(wd_path) #setear wd_path como WD
set.seed(1999) #random seed
library(tidyverse) #ggplot2, dplyr, tidyr, ggpubr, readr...
library(stats) #ops. básicas de estadística
library(factoextra) #
library(pheatmap) #heatmaps
library(gtsummary) #tabla est.descriptiva
library(MASS)
library(glmnet)
library(ggplot2)
library(gridExtra)
df <- read.csv("1_data/Dataset expresión genes.csv") # dataframe con todas las variables
#na.strings = este arg. dice si hay alguna cadena de texto q queramos importar como NA
df_genes <- df %>% dplyr::select(starts_with("AQ_")) # df solo de los genes
df_nogenes <- df %>% dplyr::select(-starts_with("AQ_")) # df solo de los genes
pca.result <- prcomp(df_genes, center = TRUE, scale = TRUE )
eigenvalues <- get_eigenvalue(pca.result)
eigenvalues
fviz_eig(pca.result, addlabels = TRUE)
fviz_pca_var(pca.result, col.var = "cos2", gradient.cols = c("blue","yellow", "red"), repel = TRUE, axes = c(1,2),
title = "Cos2 de variables en PC1 y PC2")
kmeans <- kmeans(t(df_genes), centers = 2)
clusters <- list()
for (i in 1:4) {
cluster.plot <- fviz_pca_var(pca.result, col.var = kmeans$cluster, gradient.cols = c("blue","green", "red"), axes = c(i,i+1),
legend.title ="Clusterizacion", repel = TRUE)
clusters[[i]] <- cluster.plot
}
todos.clusters <- grid.arrange(grobs = clusters, nrow = 3)
todos.clusters
graficos <- list()
for (i in 1:5) {
grafico <- fviz_contrib(pca.result, choice ="var", axes = i, top = 5)
graficos[[i]] <- grafico
}
graficos_varianza <- grid.arrange(grobs = graficos, nrow = 3)
kmeans2 <- kmeans(df_genes, centers = 3)
fviz_cluster(kmeans2, df_genes)
fviz_pca_ind(pca.result, col.ind = "cos2", gradient.cols = c("blue","yellow", "red"), repel = TRUE)
df$metastasisnosi <- as.factor(ifelse( df$extension == "metastasico", "metastasis", "no_metastasis"))
df_pca <- as.data.frame(pca.result$x)
df_pca <- df_pca[1:5]
componentes <- c("PC1", "PC2", "PC3", "PC4", "PC5")
componentes.plots <- list()
for (i in 1:(length(componentes) - 1))  {
grafico <- ggplot(df_pca, aes_string(x =componentes[i], y = componentes[i+1], color = df$metastasisnosi))+geom_point(size = 3)
componentes.plots[[i]] <- grafico
}
grid.arrange(grobs = componentes.plots, nrow = 3)
pcs = c("PC1", "PC2", "PC3", "PC4", "PC5")
df_pca <- df_pca %>% dplyr::select(pcs)
df_rlog =  cbind(df_pca, df_nogenes)
# selecciono vars_num
df_non_numeric <- df %>% keep(~ !is.numeric(.))
# recodificar vars factor y chr como números --> castear a as.numeric()
for (col in names(df_non_numeric)) {
if ("si" %in% col){
gsub("si", "1", col)
} else if ("no" %in% col) {
gsub("no", "0", col)
}
}
df_non_numeric
df_non_numeric <- df_non_numeric %>% gsub('si', "1", df_non_numeric)
df_non_numeric <- df_non_numeric %>% gsub('no', "0", df_non_numeric)
df_non_numeric
#Primero aplicamos LASSO
df_regresion <- cbind(df_pca, df_nogenes)
#Necesitamos que todas las variables sean numericas
variables <- colnames(df_regresion)
variables_numericas <- colnames(df_regresion[ ,sapply(df_regresion, is.numeric)]) #variables numericas
#Escalado variables numericas:
df_numerico <- df_regresion %>% dplyr::select(variables_numericas)
df_numerico <- scale(df_numerico)
df_regresion <- df_regresion %>% dplyr::select(-variables_numericas)
df_regresion <- cbind(df_regresion, df_numerico)
variables_no_numericas <- variables[!variables %in% variables_numericas] #variables no numericas
variables_no_numericas_no_sexo <- variables_no_numericas[variables_no_numericas != "sexo"] #variables de tipo si/no
for (i in variables_no_numericas_no_sexo) {
df_regresion[[i]] <- ifelse (df_regresion[[i]] == "si", 1, 0
)
}
#ahora manualmente se haria el sexo
df_regresion$sexo <- ifelse(df_regresion$sexo == "mujer", 1, 0)
df_regresion <- df_regresion %>% dplyr::select(-X,-id)
y <-  as.factor(df$metastasisnosi)
x <- as.matrix(df_regresion)
grid <- 10^seq(10,-2, length=100)
lasso.result <- glmnet :: cv.glmnet(x, y, family = "binomial", lambda = grid, alpha = 1)
lambda_min <- lasso.result$lambda.min
lasso.coef <- coef(lasso.result, s = "lambda.min")
lasso.coef <- as.data.frame(as.matrix(lasso.coef))
lasso.coef$s1 <- round(lasso.coef$s1, 5)
View(lasso.coef)
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = FALSE)
rm(list=ls()) #resetear WEnv
thisfile_path <- file.choose() #elegir
wd_path <- sub("/[^/]+$", "", thisfile_path) #ruta de este archivo hasta el último "/" (excluyendo nombre del archivo))
setwd(wd_path) #setear wd_path como WD
set.seed(1999) #random seed
library(tidyverse) #ggplot2, dplyr, tidyr, ggpubr, readr...
library(stats) #ops. básicas de estadística
library(factoextra) #
library(pheatmap) #heatmaps
library(gtsummary) #tabla est.descriptiva
library(MASS)
library(glmnet)
library(ggplot2)
library(gridExtra)
df <- read.csv("1_data/Dataset expresión genes.csv") # dataframe con todas las variables
#na.strings = este arg. dice si hay alguna cadena de texto q queramos importar como NA
df_genes <- df %>% dplyr::select(starts_with("AQ_")) # df solo de los genes
df_nogenes <- df %>% dplyr::select(-starts_with("AQ_")) # df solo de los genes
pca.result <- prcomp(df_genes, center = TRUE, scale = TRUE )
eigenvalues <- get_eigenvalue(pca.result)
eigenvalues
fviz_eig(pca.result, addlabels = TRUE)
fviz_pca_var(pca.result, col.var = "cos2", gradient.cols = c("blue","yellow", "red"), repel = TRUE, axes = c(1,2),
title = "Cos2 de variables en PC1 y PC2")
kmeans <- kmeans(t(df_genes), centers = 2)
clusters <- list()
for (i in 1:4) {
cluster.plot <- fviz_pca_var(pca.result, col.var = kmeans$cluster, gradient.cols = c("blue","green", "red"), axes = c(i,i+1),
legend.title ="Clusterizacion", repel = TRUE)
clusters[[i]] <- cluster.plot
}
todos.clusters <- grid.arrange(grobs = clusters, nrow = 3)
todos.clusters
graficos <- list()
for (i in 1:5) {
grafico <- fviz_contrib(pca.result, choice ="var", axes = i, top = 5)
graficos[[i]] <- grafico
}
graficos_varianza <- grid.arrange(grobs = graficos, nrow = 3)
kmeans2 <- kmeans(df_genes, centers = 3)
fviz_cluster(kmeans2, df_genes)
fviz_pca_ind(pca.result, col.ind = "cos2", gradient.cols = c("blue","yellow", "red"), repel = TRUE)
df$metastasisnosi <- as.factor(ifelse( df$extension == "metastasico", "metastasis", "no_metastasis"))
df_pca <- as.data.frame(pca.result$x)
df_pca <- df_pca[1:5]
componentes <- c("PC1", "PC2", "PC3", "PC4", "PC5")
componentes.plots <- list()
for (i in 1:(length(componentes) - 1))  {
grafico <- ggplot(df_pca, aes_string(x =componentes[i], y = componentes[i+1], color = df$metastasisnosi))+geom_point(size = 3)
componentes.plots[[i]] <- grafico
}
grid.arrange(grobs = componentes.plots, nrow = 3)
# las variables factor están en tipo character, tengo que convertirlas a numérico pq glmnet no admite tipo factor
# casteo a numérico variables char
df_nogenes <- df %>% dplyr::select(-starts_with("AQ_"), -X, -id, -extension) # df de vars q no son genes
df_char <- df_nogenes[ ,sapply(df_nogenes, is.character)]# guardamos las vars factor en un vector lógico
df_char$antiemesis <- as.character(ifelse(df$corticoides=="antiemesis", "1", "0"))
df_char$corticoides <- as.character(ifelse(df$corticoides=="si", "1", "0"))
df_nochar <- df_nogenes[, sapply(df_nogenes, is.numeric)]
df_nochar <- as.data.frame(scale(df_nochar, center=TRUE, scale=TRUE))
# convierto variables tipo char a factor y luego a numeros para codificarlas de manera que glmnet las entienda
for (i in colnames(df_char)) {
df_char[[i]] <-as.numeric(as.factor(df_char[[i]]))
}
# y concateno df_char, df_nochar y df5 pcs en df_lasso
df_lasso <- cbind(df_char, df_nochar)
# ahora tb dejo bien formateada la variable respuesta
df$metastasis = as.factor(as.numeric(ifelse(df$extension == "metastasico", 1, 0)))
# defino variables para df_lasso
cols = c(colnames(df_lasso))
x <- df_lasso[, cols]
y <- df$metastasis
# convertir a matriz para incluir interacciones entre todas las variables
#formula <- as.formula(paste("y ~", paste(names(x), collapse = " * "))) # Crear una matriz de diseño con interacciones entre todas las variables
# construyo un grid para lambda (para el lambda optimo??)
grid <- 10^seq(1.5, -1.5, length=100)
ridge <- glmnet(x, y, alpha=0, lambda=grid, family="binomial")
dim(coef(ridge))
print(coef(ridge))
plot(ridge, label=TRUE) # lambda plot
plot(ridge, xvar="lambda", label=TRUE) #log lambda plot
x_matrix <- as.matrix(x)
y_matrix <- as.matrix(y)
# hago un m
#modelo de cross validation
ridge_cv <- cv.glmnet(x_matrix, y_matrix, alpha=0, lambda=grid, family="binomial")
lambda_min <- ridge_cv$lambda.min # este es el lambda que menor error nos da para el modelo, me lo guardo en un objeto
# y lo uso para generar modelo de ridge
ridge_cv <- glmnet(x_matrix, y_matrix, alpha=0, lambda = lambda_min, family="binomial")
coef(ridge_cv)
grid <- 10^seq(-4,1, lenght=100)
lasso <- glmnet(x, y, alpha=1, lambda=grid, family="binomial")
dim(coef(lasso))
print(coef(lasso))
plot(lasso, label=TRUE) # lambda plot
plot(lasso, xvar="lambda", label=TRUE) #log lambda plot
# hago un modelo de cross validation
grid <- 10^seq(-3,0, lenght=100)
lasso_cv <- cv.glmnet(x_matrix, y_matrix, alpha=1, lambda=grid, family="binomial")
plot(lasso_cv)
lambda_min <- lasso_cv$lambda.min
lambda_1se <- lasso_cv$lambda.1se # este es el lambda que menor error nos da para el modelo, me lo guardo en un objeto
# y lo uso para generar modelo de ridge
lasso_cv <- glmnet(x, y, alpha=1, lambda = lambda_1se, family="binomial")
coef(lasso_cv)
seq_alpha <- seq(0,1,by=0.01)
grid <- 10^seq(3, -3, length=100) # lo hago de + a - pq el lambda óptimo  es negativo (ver plots iniciales de lasso)
best_alpha <- NULL
best_lambda <- NULL
min_error <- Inf
# encontrar el mejor alpha
for (alpha in seq_alpha) {
enet_cv <- cv.glmnet(x_matrix, y_matrix, alpha=alpha, lambda=grid, family="binomial")
# Obtener valor de lambda óptimo elegido automáticamente
lambda_min<-enet_cv$lambda.min
#Obtener el error de validación cruzada mínimo
cv_error <- min(enet_cv$cvm)
#Actualizar el mejor alpha y lambda si se encuentra un error de cv menor
if (cv_error < min_error) {
min_error <- cv_error
best_alpha <- alpha
best_lambda <- lambda_min
}}
# Imprimir el mejor alpha y lambda encontrados tras el bucle
cat("Mejor alpha:", best_alpha, "\n")
cat("Mejor lambda:", best_lambda, "\n")
grid <- 10^seq(0, -2, length=100)
# hago un modelo de cross validation
enet_cv <- cv.glmnet(x_matrix, y_matrix, alpha=0.67, lambda=grid, family="binomial")
plot(enet_cv)
lambda_1se <- enet_cv$lambda.1se
lambda_1se
